<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Webcam Emotion Detection</title>
    <link href="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.css" rel="stylesheet">
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <h1>Webcam Emotion Detection</h1>
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="upload-img.html">Upload Image</a></li>
        </ul>
    </nav>
    <div id="liveView">
        <video id="video" autoplay playsinline></video>
        <canvas id="output_canvas" width="640" height="480"></canvas>
    </div>

    <div class="blend-shapes">
        <h2>Blend Shapes</h2>
        <ul class="blend-shapes-list" id="video-blend-shapes"></ul>
    </div>

    <div id="emotionResult">
        <p>Predicted Emotion: <span id="predicted-emotion">N/A</span></p>
        <p>Confidence: <span id="confidence">N/A</span></p>
    </div>      

    <div class="controls">
        <button id="startButton">Start Camera</button>
        <button id="stopButton">Stop Camera</button>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils@latest/camera_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils@latest/control_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils@latest/drawing_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh@latest/face_mesh.js" crossorigin="anonymous"></script>

    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('output_canvas');
        const canvasCtx = canvas.getContext('2d');

        let runningModel = false;
        let camera; 

        // Endpoint para realizar a predição de emoção
        const predictEmotionEndpoint = '/detectEmotionFromBlendshapes';

        // Configuração do MediaPipe FaceMesh
        const faceMesh = new FaceMesh({locateFile: (file) => {
            return `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`;
        }});
        faceMesh.onResults(onResults);

        faceMesh.onLoad = () => {
            console.log('FaceMesh model loaded');
            startDetection();
        };

        faceMesh.setOptions({
            maxNumFaces: 1,
            refineLandmarks: true,
            minDetectionConfidence: 0.5,
            minTrackingConfidence: 0.5
        });

        faceMesh.onError = (error) => {
            console.error('FaceMesh error:', error);
        };

        // Função para processar os resultados do MediaPipe
        function onResults(results) {
            console.log('Results received:', results);
            canvasCtx.save();
            canvasCtx.clearRect(0, 0, canvas.width, canvas.height);
            canvasCtx.drawImage(results.image, 0, 0, canvas.width, canvas.height);

            if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
                for (const landmarks of results.multiFaceLandmarks) {
                    drawBlendShapes(results);
                    
                    // Enviar blendshapes para o back-end
                    sendBlendShapes(landmarks);
                }
            } else {
                console.log('No landmarks detected.');
            }
            canvasCtx.restore();
        }        

        // Função para desenhar blendshapes
        function drawBlendShapes(results) {
            const blendShapes = results.multiFaceLandmarks[0].blendshapes || {};
            const blendShapeList = document.getElementById('video-blend-shapes');
            blendShapeList.innerHTML = '';

            Object.keys(blendShapes).forEach(shape => {
                const listItem = document.createElement('li');
                listItem.textContent = `${shape}: ${blendShapes[shape].toFixed(2)}`;
                blendShapeList.appendChild(listItem);
            });
        }        

        // Função para enviar os blendshapes para o back-end
        async function sendBlendShapes(faceLandmarks) {
            console.log('Sending blendshapes to backend');
            const blendShapes = faceLandmarks.blendshapes || {};

            try {
                const response = await fetch(predictEmotionEndpoint, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify(blendShapes), // Envia a lista de blendshapes diretamente
                });
        
                console.log('Response received:', response.status);
        
                if (response.ok) {
                    const prediction = await response.json();
                    updateUI(prediction);
                } else {
                    console.error('Erro na predição:', response.statusText);
                }
            } catch (error) {
                console.error('Erro ao enviar blendshapes:', error);
            }
        }

        // Atualiza a interface do usuário com a emoção predita
        function updateUI(prediction) {
            const emotionElement = document.getElementById('predicted-emotion');
            const confidenceElement = document.getElementById('confidence');

            if (prediction) {
                emotionElement.textContent = prediction.emotion || 'UNKNOWN';

                // Exibir as probabilidades de cada emoção
                const emotionProbabilities = Object.keys(prediction).filter(key => key.startsWith('probability('));
                if (emotionProbabilities.length > 0) {
                    const probabilities = emotionProbabilities.map(key => `${key.replace('probability(', '').replace(')', '')}: ${(prediction[key] * 100).toFixed(2)}%`).join(', ');
                    confidenceElement.textContent = probabilities;
                } else {
                    confidenceElement.textContent = 'N/A';
                }
            }
        }

        // Função para iniciar a detecção de vídeo
        async function startDetection() {
            runningModel = true;
            camera = new Camera(video, {
                onFrame: async () => runningModel && await faceMesh.send({ image: video }),
                width: 640,
                height: 480
            });
            camera.start();
        }

        // Função para parar a detecção de vídeo
        function stopDetection() {
            runningModel = false;
            if (camera) {
                camera.stop();
                camera = null;
                video.srcObject?.getTracks().forEach(track => track.stop());
                video.srcObject = null;
                canvasCtx.clearRect(0, 0, canvas.width, canvas.height);
            } else {
                console.warn('No camera instance to stop');
            }
        }

        // Event listeners para os botões de controle da câmera
        document.getElementById('startButton').addEventListener('click', startDetection);
        document.getElementById('stopButton').addEventListener('click', stopDetection);
    </script>
</body>
</html>