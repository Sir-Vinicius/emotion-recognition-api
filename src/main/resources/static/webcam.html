<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Webcam Emotion Detection</title>
    <link href="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.css" rel="stylesheet">
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <h1>Webcam Emotion Detection</h1>
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="upload-img.html">Upload Image</a></li>
        </ul>
    </nav>
    <div id="liveView">
        <video id="video" autoplay playsinline></video>
        <canvas id="output_canvas" width="640" height="480"></canvas>
    </div>

    <div class="blend-shapes">
        <h2>Facial Landmarks</h2>
        <ul class="blend-shapes-list" id="video-blend-shapes"></ul>
    </div>

    <div id="emotionResult">
        <p>Predicted Emotion: <span id="predicted-emotion">N/A</span></p>
        <p>Confidence: <span id="confidence">N/A</span></p>
    </div>      

    <div class="controls">
        <button id="startButton">Start Camera</button>
        <button id="stopButton">Stop Camera</button>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils@latest/camera_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils@latest/control_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils@latest/drawing_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh@latest/face_mesh.js" crossorigin="anonymous"></script>

    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('output_canvas');
        const canvasCtx = canvas.getContext('2d');

        let runningModel = false;
        let camera; 

        // Endpoint para realizar a predição de emoção
        const predictEmotionEndpoint = '/detectEmotionFromLandmarks';

        // Configuração do MediaPipe FaceMesh
        const faceMesh = new FaceMesh({locateFile: (file) => {
            return `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`;
        }});
        faceMesh.onResults(onResults);

        faceMesh.onLoad = () => {
            console.log('FaceMesh model loaded');
            startDetection();
        };

        faceMesh.setOptions({
            maxNumFaces: 1,
            refineLandmarks: true,
            minDetectionConfidence: 0.5,
            minTrackingConfidence: 0.5
        });

        faceMesh.onError = (error) => {
            console.error('FaceMesh error:', error);
        };

        // Lista de linhas para desenhar
        const listLines = [
            [130, 27], [130, 23], [27, 133], [23, 133], // olho esquerdo
            [359, 257], [359, 253], [257, 362], [253, 362], // olho direito
            [130, 70], [107, 168], [168, 336], [300, 359], // sobrancelhas e nariz
            [168, 4], // nariz
            [133, 4], [362, 4], // olhos - ponta nariz
            [4, 50], [4, 280], // ponta nariz - bochecha
            [4, 0], // ponta nariz - topo boca
            [70, 105], [105, 107], [107, 108], [108, 104], [104, 71], [71, 70], // sobrancelha esquerda
            [336, 334], [334, 300], [300, 301], [301, 333], [333, 337], [337, 336], // sobrancelha direita
            [61, 40], [61, 91], [40, 0], [91, 17], [0, 270], [17, 321], [270, 291], [321, 291], // contorno da boca
            [50, 61], [280, 291], // bochechas
            [212, 216], [216, 214], [214, 207], [207, 192], [192, 197], // lado boca esquerda zig zag
            [432, 436], [436, 434], [434, 427], [427, 416], [416, 411], // lado boca direita zig zag
            [61, 212], [291, 432]
        ];

        // Função para processar os resultados do MediaPipe
        function onResults(results) {
            console.log('Results received:', results);
            canvasCtx.save();
            canvasCtx.clearRect(0, 0, canvas.width, canvas.height);
            canvasCtx.drawImage(results.image, 0, 0, canvas.width, canvas.height);
        
            if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
                for (const landmarks of results.multiFaceLandmarks) {
                    drawConnectors(canvasCtx, landmarks, FACEMESH_TESSELATION, { color: '#C0C0C070', lineWidth: 1 });
                    drawCustomLandmarks(canvasCtx, landmarks);
        
                    // Enviar landmarks para o back-end
                    sendLandmarks(landmarks);
                }
            } else {
                console.log('No landmarks detected.');
            }
            canvasCtx.restore();
        }        

        // Função para desenhar linhas e pontos personalizados
        function drawCustomLandmarks(ctx, landmarks) {
            const points = landmarks.map(landmark => ({
                x: landmark.x * canvas.width,
                y: landmark.y * canvas.height
            }));
        
            console.log('Drawing landmarks:', points);
        
            // Desenhar as linhas com base na lista de linhas
            ctx.strokeStyle = '#FF0000'; // Cor da linha
            ctx.lineWidth = 2; // Espessura da linha
            ctx.beginPath();
        
            listLines.forEach(line => {
                const [startIndex, endIndex] = line;
                const startPoint = points[startIndex];
                const endPoint = points[endIndex];
                if (startPoint && endPoint) {
                    ctx.moveTo(startPoint.x, startPoint.y);
                    ctx.lineTo(endPoint.x, endPoint.y);
                }
            });
        
            ctx.stroke();
        
            // Desenhar pontos nos landmarks
            ctx.fillStyle = '#00FF00'; // Cor do ponto
            points.forEach(point => {
                ctx.beginPath();
                ctx.arc(point.x, point.y, 3, 0, 2 * Math.PI);
                ctx.fill();
            });
        }        

        // Função para enviar os landmarks para o back-end
        async function sendLandmarks(landmarks) {
            console.log('Sending landmarks to backend');
            const formattedLandmarks = landmarks.map(landmark => ({
                x: landmark.x,
                y: landmark.y
            }));

            try {
                const response = await fetch(predictEmotionEndpoint, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify(formattedLandmarks), // Envia diretamente a lista de landmarks
                });

                console.log('Response received:', response.status);

                if (response.ok) {
                    const prediction = await response.json();
                    updateUI(prediction);
                } else {
                    console.error('Erro na predição:', response.statusText);
                }
            } catch (error) {
                console.error('Erro ao enviar landmarks:', error);
            }
        }


        // Atualiza a interface do usuário com a emoção predita
        function updateUI(prediction) {
            const emotionElement = document.getElementById('predicted-emotion');
            const confidenceElement = document.getElementById('confidence');

            if (prediction) {
                // Mapeia o número da emoção para string no frontend
                const emotionMap = {
                    0: 'NEUTRAL',
                    1: 'HAPPINESS',
                    2: 'SADNESS/ANGER',
                    3: 'SURPRISE'
                };

                emotionElement.textContent = emotionMap[prediction.emotion] || 'UNKNOWN';
                confidenceElement.textContent = prediction.confidence.toFixed(2);
            }
        }

        // Função para iniciar a detecção de vídeo
        async function startDetection() {
            runningModel = true;
            camera = new Camera(video, {
                onFrame: async () => runningModel && await faceMesh.send({ image: video }),
                width: 640,
                height: 480
            });
            camera.start();
        }

        // Função para parar a detecção de vídeo
        function stopDetection() {
            runningModel = false;
            if (camera) {
                camera.stop();
                camera = null;
                video.srcObject?.getTracks().forEach(track => track.stop());
                video.srcObject = null;
                canvasCtx.clearRect(0, 0, canvas.width, canvas.height);
            } else {
                console.warn('No camera instance to stop');
            }
        }

        // Event listeners para os botões de controle da câmera
        document.getElementById('startButton').addEventListener('click', startDetection);
        document.getElementById('stopButton').addEventListener('click', stopDetection);
    </script>
</body>
</html>